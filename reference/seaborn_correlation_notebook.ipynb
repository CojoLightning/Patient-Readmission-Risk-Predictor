{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Seaborn-based Correlation Exploration\n",
    "\n",
    "Helper notebook to explore correlations between variables using seaborn visualizations.\n",
    "\n",
    "Features:\n",
    "- Safe data loading (treats `?` as missing / NaN)\n",
    "- Correlation matrix computation (Pearson / Spearman / Kendall)\n",
    "- Heatmap + clustermap for correlation structure\n",
    "- Pairplots (sampled) and jointplots for pairwise relationships\n",
    "- Categorical vs target heatmaps\n",
    "- Utility to compute correlations with a binary/numeric target\n",
    "\n",
    "Usage: update DATA_PATH to point to your CSV (default `diabetic_data.csv`), run the cells, then run the example usage cell at the bottom.\n"
   ],
   "id": "b1752873783a61e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install dependencies if needed (uncomment to run once)\n",
    "# !pip install pandas numpy seaborn matplotlib scikit-learn\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats  # new: statistics for chi2 / pointbiserial\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "DEFAULT_CMAP = \"vlag\"\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"C:/Users/reinacherc/Downloads/diabetes+130-us+hospitals+for+years+1999-2008 (1)/diabetic_data.csv\"  # change to your path\n",
    "PLOT_DIR = \"../eda_plots\"\n",
    "RANDOM_STATE = 42\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n"
   ],
   "id": "7fcd2f4dec2c8f2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def compute_numeric_columns(df: pd.DataFrame) -> List[str]:\n",
    "    # return the list of columns with numeric dtypes (used for correlation calculations)\n",
    "    return df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "def corr_matrix(df: pd.DataFrame, cols: Optional[List[str]] = None, method: str = \"pearson\") -> pd.DataFrame:\n",
    "    # if no columns provided, automatically pick numeric columns\n",
    "    if cols is None:\n",
    "        cols = compute_numeric_columns(df)\n",
    "    # compute pairwise correlation matrix using requested method\n",
    "    mat = df[cols].corr(method=method)\n",
    "    return mat\n",
    "\n",
    "def plot_corr_heatmap(\n",
    "    corr: pd.DataFrame,\n",
    "    title: str = \"Correlation heatmap\",\n",
    "    annot: bool = True,\n",
    "    fmt: str = \".2f\",\n",
    "    vmax: float = 1.0,\n",
    "    vmin: float = -1.0,\n",
    "    cmap: str = DEFAULT_CMAP,\n",
    "    fname: Optional[str] = None,\n",
    "    figsize: Tuple[int, int] = (12, 10),\n",
    "    mask_upper: bool = True,\n",
    "):\n",
    "    # create a figure sized for readability\n",
    "    plt.figure(figsize=figsize)\n",
    "    # optionally mask upper triangle to avoid duplicate correlations in symmetric matrix\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool)) if mask_upper else None\n",
    "    # draw heatmap: center at 0 makes positive/negative correlations visually symmetric\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        mask=mask,\n",
    "        cmap=cmap,\n",
    "        vmax=vmax,\n",
    "        vmin=vmin,\n",
    "        center=0,\n",
    "        square=False,\n",
    "        linewidths=0.5,\n",
    "        annot=annot,\n",
    "        fmt=fmt,\n",
    "        cbar_kws={\"shrink\": 0.6},\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    # save to file if filename provided\n",
    "    if fname:\n",
    "        plt.savefig(fname, dpi=150)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_clustermap(\n",
    "    corr: pd.DataFrame,\n",
    "    title: str = \"Correlation clustermap (hierarchical)\",\n",
    "    cmap: str = DEFAULT_CMAP,\n",
    "    fname: Optional[str] = None,\n",
    "    figsize: Tuple[int, int] = (12, 12),\n",
    "):\n",
    "    # clustermap performs hierarchical clustering on correlation matrix to show blocks\n",
    "    try:\n",
    "        cg = sns.clustermap(\n",
    "            corr,\n",
    "            cmap=cmap,\n",
    "            center=0,\n",
    "            figsize=figsize,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.6},\n",
    "        )\n",
    "        # set a suptitle slightly above the figure\n",
    "        cg.fig.suptitle(title, y=1.02)\n",
    "        # save the whole clustermap figure if requested\n",
    "        if fname:\n",
    "            cg.savefig(fname, dpi=150)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        # clustermap can fail with very large matrices or missing numeric conversions\n",
    "        print(\"Clustermap failed:\", e)\n",
    "\n",
    "def pairplot_sample(\n",
    "    df: pd.DataFrame,\n",
    "    cols: List[str],\n",
    "    hue: Optional[str] = None,\n",
    "    sample_frac: float = 0.2,\n",
    "    sample_n: Optional[int] = None,\n",
    "    diag_kind: str = \"hist\",\n",
    "    kind: str = \"scatter\",\n",
    "    fname: Optional[str] = None,\n",
    "    plot_kws: dict = None,\n",
    "):\n",
    "    # build the sample DataFrame: include hue column if provided and present\n",
    "    if sample_n is not None:\n",
    "        data = df[cols + ([hue] if hue and hue in df.columns else [])].dropna().sample(n=min(sample_n, len(df)), random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        frac = min(max(sample_frac, 0.0), 1.0)\n",
    "        data = df[cols + ([hue] if hue and hue in df.columns else [])].dropna().sample(frac=frac, random_state=RANDOM_STATE)\n",
    "    # default plotting kwargs: small points and some transparency help with overplotting\n",
    "    plot_kws = plot_kws or {\"s\": 15, \"alpha\": 0.6}\n",
    "    try:\n",
    "        # seaborn.pairplot shows pairwise relationships and marginal distributions\n",
    "        pp = sns.pairplot(data, vars=cols, hue=hue, diag_kind=diag_kind, kind=kind, plot_kws=plot_kws)\n",
    "        if fname:\n",
    "            pp.fig.savefig(fname, dpi=150)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        # pairplot can be memory-intensive; provide a helpful message on failure\n",
    "        print(\"Pairplot failed (maybe too many features or memory constraints):\", e)\n",
    "\n",
    "def jointplot_pair(df: pd.DataFrame, x: str, y: str, hue: Optional[str] = None, kind: str = \"reg\", fname: Optional[str] = None):\n",
    "    # prepare data for jointplot, including hue if valid\n",
    "    data = df[[x, y] + ([hue] if hue and hue in df.columns else [])].dropna()\n",
    "    try:\n",
    "        # seaborn.jointplot gives a main plot plus marginals; hue support varies by seaborn version\n",
    "        jp = sns.jointplot(data=data, x=x, y=y, hue=hue, kind=kind, height=7, marginal_kws=dict(bins=30))\n",
    "        if fname:\n",
    "            jp.fig.savefig(fname, dpi=150)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        # fallback to a simple scatter + title when jointplot with hue/kind fails\n",
    "        print(\"jointplot failed with hue or kind; falling back to scatter + marginals:\", e)\n",
    "        sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=0.6)\n",
    "        plt.title(f\"{x} vs {y}\")\n",
    "        if fname:\n",
    "            plt.savefig(fname, dpi=150)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def categorical_correlation_heatmap(df: pd.DataFrame, cat_cols: List[str], target: Optional[str] = None, fname: Optional[str] = None, top_n: int = 30, normalize: bool = True):\n",
    "    # for each categorical column, either show normalized crosstab against target or show top categories\n",
    "    # top_n controls how many top categories to display for barplots; normalize toggles crosstab normalization\n",
    "    for c in cat_cols:\n",
    "        if target and target in df.columns:\n",
    "            # build crosstab: normalize by index if requested, otherwise show raw counts\n",
    "            if normalize:\n",
    "                ct = pd.crosstab(df[c], df[target], normalize=\"index\").fillna(0)\n",
    "                fmt = \".2f\"\n",
    "                cbar_label = \"fraction\"\n",
    "            else:\n",
    "                ct = pd.crosstab(df[c], df[target]).fillna(0)\n",
    "                fmt = \"d\"\n",
    "                cbar_label = \"count\"\n",
    "            title = f\"{'Normalized ' if normalize else ''}distribution of {c} by {target}\"\n",
    "            plt.figure(figsize=(8, max(3, ct.shape[0] * 0.25)))\n",
    "            sns.heatmap(ct, cmap=\"Blues\", annot=True, fmt=fmt, cbar_kws={\"label\": cbar_label})\n",
    "            plt.title(title)\n",
    "            plt.ylabel(c)\n",
    "            plt.xlabel(target)\n",
    "            if fname:\n",
    "                base, ext = os.path.splitext(fname)\n",
    "                out = f\"{base}_{c}{ext or '.png'}\"\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out, dpi=150)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        else:\n",
    "            # if no target provided, show the top categories by count (limited by top_n)\n",
    "            counts = df[c].value_counts().head(top_n)\n",
    "            plt.figure(figsize=(8, max(3, len(counts) * 0.25)))\n",
    "            sns.barplot(y=counts.index, x=counts.values, palette=\"muted\")\n",
    "            plt.title(f\"Top {top_n} categories for {c}\")\n",
    "            if fname:\n",
    "                base, ext = os.path.splitext(fname)\n",
    "                out = f\"{base}_{c}{ext or '.png'}\"\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out, dpi=150)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "def corr_with_target_table(df: pd.DataFrame, target: str = \"readmit_30\", methods: List[str] = [\"pearson\", \"spearman\"]) -> pd.DataFrame:\n",
    "    # compute correlations of numeric features with a given target using requested methods\n",
    "    numeric = compute_numeric_columns(df)\n",
    "    if target not in df.columns:\n",
    "        raise KeyError(f\"Target {target} not in dataframe columns.\")\n",
    "    results = {}\n",
    "    for m in methods:\n",
    "        # append target to numeric list to ensure it's included in correlation matrix\n",
    "        mat = corr_matrix(df, cols=numeric + [target], method=m)\n",
    "        # take the column corresponding to target and drop the self-correlation row\n",
    "        results[m] = mat[target].drop(index=target)\n",
    "    result_df = pd.DataFrame(results)\n",
    "    # sort by absolute Pearson correlation if available, otherwise by first method provided\n",
    "    if \"pearson\" in result_df.columns:\n",
    "        result_df = result_df.reindex(result_df[\"pearson\"].abs().sort_values(ascending=False).index)\n",
    "    else:\n",
    "        result_df = result_df.reindex(result_df.iloc[:, 0].abs().sort_values(ascending=False).index)\n",
    "    return result_df\n"
   ],
   "id": "1d17182400ea0494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV and treat '?' as NaN. Returns a DataFrame with stripped column names.\"\"\"\n",
    "    # ensure the path exists so users get a clear error message if misconfigured\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {path}\")\n",
    "    # read CSV and convert '?' into NaN immediately to avoid treating them as valid strings\n",
    "    df = pd.read_csv(path, low_memory=False, na_values=['?'])\n",
    "    # normalize column names by stripping whitespace\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    # extra safety: replace any remaining '?' strings with NaN\n",
    "    df = df.replace('?', np.nan)\n",
    "    # print summary so user knows file loaded and replacements performed\n",
    "    print(f\"Loaded {len(df)} rows and {len(df.columns)} columns. Replaced '?' with NaN.\")\n",
    "    return df\n"
   ],
   "id": "9f036f294fcfa257"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# New helpers: convert target to binary and compute associations per feature.\n",
    "\n",
    "def _make_binary_target(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert a target series into a 0/1 binary Series for 'readmission' semantics.\n",
    "    - If numeric and only two unique values, keep 0/1 after normalization.\n",
    "    - If values include '<30' treat '<30' as positive (1).\n",
    "    - If values include recognizable yes-like tokens, treat them as positive.\n",
    "    - If exactly two unique string values, map the second value to 1.\n",
    "    - Otherwise raise ValueError to force explicit handling.\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    uniq = pd.Series(s.unique()).astype(str).tolist()\n",
    "    # numeric two-valued -> map to 0/1\n",
    "    if pd.api.types.is_numeric_dtype(s) and len(uniq) == 2:\n",
    "        # normalize to 0/1\n",
    "        mapped = (series.astype(float) - series.min()) / (series.max() - series.min())\n",
    "        return mapped.fillna(0).astype(int)\n",
    "    # common label for 30-day readmit in diabetes dataset\n",
    "    if \"<30\" in uniq:\n",
    "        return series.astype(str).map(lambda x: 1 if str(x) == \"<30\" else 0).astype(int)\n",
    "    # yes/no style\n",
    "    yes_tokens = {\"yes\", \"y\", \"true\", \"1\", \"positive\"}\n",
    "    if any(token.lower() in yes_tokens for token in uniq):\n",
    "        return series.astype(str).map(lambda x: 1 if str(x).lower() in yes_tokens else 0).astype(int)\n",
    "    # fallback: if two unique strings map second -> 1\n",
    "    if len(uniq) == 2:\n",
    "        second = uniq[1]\n",
    "        return series.astype(str).map(lambda x: 1 if str(x) == second else 0).astype(int)\n",
    "    # cannot safely binarize automatically\n",
    "    raise ValueError(\"Cannot automatically convert target to binary. Provide a binary column or preprocess target.\")\n",
    "\n",
    "def compute_feature_target_associations(df: pd.DataFrame, target: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute associations between each column in df and the given target column.\n",
    "    - Numeric columns: point-biserial correlation vs binary target (returns r and p-value)\n",
    "    - Categorical columns: Cramér's V vs binary target (returns V and p-value from chi2)\n",
    "    Returns a DataFrame sorted by absolute 'association' descending.\n",
    "    \"\"\"\n",
    "    if target not in df.columns:\n",
    "        raise KeyError(f\"Target {target} not in dataframe columns.\")\n",
    "    # build binary target\n",
    "    try:\n",
    "        y = _make_binary_target(df[target])\n",
    "    except ValueError as e:\n",
    "        # bubble up a clearer error for users\n",
    "        raise ValueError(f\"Unable to convert target '{target}' to binary automatically: {e}\")\n",
    "\n",
    "    results = []\n",
    "    n = len(df)\n",
    "    numeric_cols = compute_numeric_columns(df)\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    # numeric associations (point-biserial)\n",
    "    for col in numeric_cols:\n",
    "        # align and drop missing pairs\n",
    "        sub = df[[col]].join(y.rename(\"_target\")).dropna()\n",
    "        if sub.shape[0] < 3:\n",
    "            results.append({\"feature\": col, \"type\": \"numeric\", \"association\": np.nan, \"p_value\": np.nan, \"method\": \"pointbiserial\"})\n",
    "            continue\n",
    "        try:\n",
    "            # pointbiserialr expects binary first, continuous second\n",
    "            r, p = stats.pointbiserialr(sub[\"_target\"].astype(int), sub[col].astype(float))\n",
    "        except Exception:\n",
    "            # fallback to pandas corr (less informative about p)\n",
    "            r = sub[col].astype(float).corr(sub[\"_target\"].astype(int))\n",
    "            p = np.nan\n",
    "        results.append({\"feature\": col, \"type\": \"numeric\", \"association\": float(r) if pd.notna(r) else np.nan, \"p_value\": float(p) if pd.notna(p) else np.nan, \"method\": \"pointbiserial\"})\n",
    "\n",
    "    # categorical associations (Cramér's V)\n",
    "    for col in cat_cols:\n",
    "        # contingency table between feature and binary target\n",
    "        ct = pd.crosstab(df[col], y)\n",
    "        if ct.values.sum() == 0 or ct.shape[0] < 2:\n",
    "            results.append({\"feature\": col, \"type\": \"categorical\", \"association\": np.nan, \"p_value\": np.nan, \"method\": \"cramers_v\"})\n",
    "            continue\n",
    "        try:\n",
    "            chi2, p, dof, expected = stats.chi2_contingency(ct, correction=False)\n",
    "            n_obs = ct.values.sum()\n",
    "            r_dim, k_dim = ct.shape\n",
    "            denom = n_obs * min(r_dim - 1, k_dim - 1)\n",
    "            if denom > 0:\n",
    "                cramers_v = np.sqrt(chi2 / denom)\n",
    "            else:\n",
    "                cramers_v = np.nan\n",
    "        except Exception:\n",
    "            cramers_v = np.nan\n",
    "            p = np.nan\n",
    "        results.append({\"feature\": col, \"type\": \"categorical\", \"association\": float(cramers_v) if pd.notna(cramers_v) else np.nan, \"p_value\": float(p) if pd.notna(p) else np.nan, \"method\": \"cramers_v\"})\n",
    "\n",
    "    out_df = pd.DataFrame(results).set_index(\"feature\")\n",
    "    # Add absolute association column to sort\n",
    "    out_df[\"abs_association\"] = out_df[\"association\"].abs()\n",
    "    out_df = out_df.sort_values(\"abs_association\", ascending=False)\n",
    "    # drop helper column before returning/display\n",
    "    out_df = out_df.drop(columns=[\"abs_association\"])\n",
    "    return out_df\n"
   ],
   "id": "20c632fbef9219be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Add widget imports and availability check so notebook degrades gracefully if ipywidgets missing\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    # keep a flag and print a helpful message; widget UI won't be shown when not available\n",
    "    widgets = None\n",
    "    display = lambda *a, **k: None  # no-op\n",
    "    clear_output = lambda *a, **k: None\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    print(\"ipywidgets not available. Install with 'pip install ipywidgets' and enable the extension if needed:\", e)\n",
    "\n",
    "# Define widget builder at top-level (clean, only widget-related code)\n",
    "def create_categorical_correlation_widget(df: pd.DataFrame, max_select: int = 6):\n",
    "    \"\"\"\n",
    "    Build and display interactive widgets to select categorical columns and a target.\n",
    "    This function only builds widgets and renders selected categorical correlation plots\n",
    "    into an Output area. It does not perform other example-usage plotting.\n",
    "    \"\"\"\n",
    "    if not WIDGETS_AVAILABLE:\n",
    "        print(\"Widgets are not available in this environment.\")\n",
    "        return\n",
    "\n",
    "    # discover categorical columns and fallback if none\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    if not cat_cols:\n",
    "        print(\"No categorical columns found in dataframe.\")\n",
    "        return\n",
    "\n",
    "    # widget controls\n",
    "    sel = widgets.SelectMultiple(\n",
    "        options=cat_cols,\n",
    "        value=tuple(cat_cols[:min(len(cat_cols), max_select)]),\n",
    "        description=\"Categories\",\n",
    "        rows=min(len(cat_cols), 12),\n",
    "    )\n",
    "    numeric_and_cat = compute_numeric_columns(df) + cat_cols\n",
    "    target_options = [\"None\"] + numeric_and_cat\n",
    "    tgt = widgets.Dropdown(options=target_options, value=\"None\", description=\"Target\")\n",
    "    top_n = widgets.IntSlider(value=30, min=1, max=100, step=1, description=\"Top N\")\n",
    "    normalize = widgets.Checkbox(value=True, description=\"Normalize (crosstab)\")\n",
    "    btn = widgets.Button(description=\"Update plots\", button_style=\"primary\")\n",
    "    out = widgets.Output(layout={\"border\": \"1px solid #ddd\"})\n",
    "\n",
    "    def on_click(b):\n",
    "        # render selected plots into the output area\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            chosen = list(sel.value)\n",
    "            chosen_target = tgt.value if tgt.value != \"None\" else None\n",
    "            categorical_correlation_heatmap(df, cat_cols=chosen, target=chosen_target, fname=None, top_n=top_n.value, normalize=normalize.value)\n",
    "\n",
    "    # register handler before display\n",
    "    btn.on_click(on_click)\n",
    "\n",
    "    # layout controls and display\n",
    "    controls = widgets.HBox([widgets.VBox([sel]), widgets.VBox([tgt, top_n, normalize, btn])])\n",
    "    display(controls, out)\n"
   ],
   "id": "94c18fa7a2480713"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Example usage: update DATA_PATH above if needed and run this cell to produce a set of correlation visualizations.\n",
    "try:\n",
    "    # attempt to load data; clear message printed by load_data on success\n",
    "    df = load_data(DATA_PATH)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    print(\"Edit DATA_PATH to point to your CSV and re-run this cell.\")\n",
    "else:\n",
    "    # numeric correlation heatmaps (Pearson & Spearman)\n",
    "    numeric_cols = compute_numeric_columns(df)\n",
    "    # Limit number of numerics to a manageable size for plotting\n",
    "    if len(numeric_cols) > 60:\n",
    "        # pick top numeric columns by non-NA counts to avoid plotting extremely wide matrices\n",
    "        numeric_cols = sorted(numeric_cols, key=lambda c: df[c].notna().sum(), reverse=True)[:60]\n",
    "    corr_p = corr_matrix(df, cols=numeric_cols, method=\"pearson\")\n",
    "    plot_corr_heatmap(corr_p, title=\"Pearson correlation heatmap (numeric features)\", fname=os.path.join(PLOT_DIR, \"pearson_corr_heatmap.png\"))\n",
    "    corr_s = corr_matrix(df, cols=numeric_cols, method=\"spearman\")\n",
    "    plot_corr_heatmap(corr_s, title=\"Spearman correlation heatmap (numeric features)\", fname=os.path.join(PLOT_DIR, \"spearman_corr_heatmap.png\"))\n",
    "\n",
    "    # Clustermap to reveal correlated blocks\n",
    "    try:\n",
    "        plot_clustermap(corr_p, fname=os.path.join(PLOT_DIR, \"corr_clustermap.png\"))\n",
    "    except Exception as e:\n",
    "        print('Clustermap generation failed:', e)\n",
    "\n",
    "    # Correlation with target (if target exists)\n",
    "    if \"readmit_30\" in df.columns:\n",
    "        corr_target_df = corr_with_target_table(df, target=\"readmit_30\")\n",
    "        corr_target_df.to_csv(os.path.join(PLOT_DIR, \"correlations_with_target.csv\"))\n",
    "        print('\\nTop correlations with readmit_30:')\n",
    "        display(corr_target_df.head(30))\n",
    "\n",
    "        # Pairplot for top features correlated with target (sampled for performance)\n",
    "        top_feats = corr_target_df.head(6).index.tolist()\n",
    "        if top_feats:\n",
    "            pairplot_sample(df, cols=top_feats, hue=\"readmit_30\", sample_frac=0.2, fname=os.path.join(PLOT_DIR, \"pairplot_top_features.png\"))\n",
    "\n",
    "    # Example jointplot for two numeric features if they exist\n",
    "    if len(numeric_cols) >= 2:\n",
    "        x = numeric_cols[0]\n",
    "        y = numeric_cols[1]\n",
    "        jointplot_pair(df, x=x, y=y, hue=(\"readmit_30\" if \"readmit_30\" in df.columns else None), kind=\"reg\", fname=os.path.join(PLOT_DIR, \"jointplot_example.png\"))\n",
    "\n",
    "    # Categorical vs target heatmaps for up to 4 categorical columns (static)\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    if categorical_cols:\n",
    "        categorical_correlation_heatmap(df, cat_cols=categorical_cols[:4], target=(\"readmit_30\" if \"readmit_30\" in df.columns else None), fname=os.path.join(PLOT_DIR, \"cat_target_heatmap.png\"))\n",
    "        # additionally launch interactive widget so user can explore arbitrary selections\n",
    "        create_categorical_correlation_widget(df)\n",
    "\n",
    "    # After prior analyses, compute per-feature associations with readmission-like target\n",
    "\n",
    "    # detect likely readmission column(s)\n",
    "    candidate_targets = [\"readmit_30\", \"readmitted\", \"readmitted_30\", \"readmit\"]\n",
    "    target_col = next((c for c in candidate_targets if c in df.columns), None)\n",
    "\n",
    "    if target_col:\n",
    "        try:\n",
    "            assoc_df = compute_feature_target_associations(df, target=target_col)\n",
    "            # save and display the top associations\n",
    "            out_csv = os.path.join(PLOT_DIR, f\"associations_with_{target_col}.csv\")\n",
    "            assoc_df.to_csv(out_csv)\n",
    "            print(f\"\\nTop associations with {target_col}:\")\n",
    "            display(assoc_df.head(30))\n",
    "            print(f\"Saved associations CSV to: {out_csv}\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to compute associations with target:\", e)\n",
    "    else:\n",
    "        print(\"No readmission candidate column found; skip feature->target association summary.\")\n",
    "\n",
    "    print(f\"\\nSaved correlation plots & CSVs to: {os.path.abspath(PLOT_DIR)}\")\n",
    "\n"
   ],
   "id": "27721863642c48c"
  }
 ]
}
